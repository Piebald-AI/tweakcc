# LDD 方法论

> Loop-Driven Development：人机协作的迭代优化

## 核心理念

```
传统：Human-in-the-loop（人类执行，AI 辅助）
LDD：Human-on-the-loop（人类定标准 + 监督，AI 执行循环）
```

**AI 的能力边界**：
- ✅ 擅长：执行、迭代、模式匹配、结构化输出
- ❌ 不擅长：判断"什么是好的"、主观质量评估

**LDD 的定位**：让人类定义验证标准，AI 负责执行优化循环。

## 适用条件

LDD 适用于**需要迭代且可验证**的任务：

| 条件 | 定义 | 不满足时 |
|------|------|----------|
| **可观测性** | 变化能被捕获为可比较的形式 | 无法形成反馈循环 |
| **可验证性** | 存在外部锚定的判断方式 | 无法判断方向对不对 |
| **可修改性** | 判断结果能有效转化为调整 | 无法迭代 |

**关键**：可验证性需要"外部锚定"（人类判断、硬指标、外部信号），纯 AI 评估不算有效验证。

---

## 核心循环 EOVM

```
E → O → V → M → E → ...

变化能被捕获 → 捕获能被判断 → 判断能指导调整 → 调整能产生新变化
```

| 环节 | 做什么 | 核心问题 |
|------|--------|---------|
| **Execute** | 执行动作 | 什么动作能产生可观测的变化？ |
| **Observe** | 捕获结果 | 如何把变化捕获为可比较的形式？ |
| **Verify** | 判断方向 | 什么信号能判断方向对不对？ |
| **Modify** | 调整目标 | 验证结果如何转化为下一轮的调整？ |

---

## 验证模式

| 模式 | 验证者 | 每轮参与 | 适用场景 |
|------|--------|---------|---------|
| **自动验证** | 硬指标 / AI 红旗检测 | 自动 | 有明确规则 |
| **人类在环** | 人类判断 | 每轮 | 依赖审美、语感、专业判断 |

如果验证依赖人类判断，人类就是闭环的必要组成部分，每轮都需要参与。

### 自动验证

**硬指标**：测试通过、Schema 校验、正则匹配、断言

**AI 红旗检测**：AI 标记明显问题，不输出分数

```json
{
  "flags": ["missing_suggestion", "vague_message"],
  "severity": "warning" | "critical",
  "needsHumanReview": true,
  "details": "消息过于模糊，缺少解决建议"
}
```

红旗检测的原则：
- 只标记高置信度的问题
- 不确定的情况推给人类
- 不假装能精确评分

### 人类在环

当验证依赖人类判断时（审美、语感、专业判断），人类是闭环的必要组成部分：
- 每轮都需要人类参与验证
- 人类判断是唯一的验证信号
- 不是"偶尔抽检"，而是"每轮参与"

---

## Modify 环节

M 不只是"改"，而是**假设-验证**的过程：

| 步骤 | 做什么 |
|------|--------|
| **归因** | 为什么当前结果不够好？ |
| **假设** | 如果改 X，应该会变好 |
| **对抗** | 第三方审视：这个修改合理吗？ |
| **修改** | 基于假设进行调整 |
| **预期** | 下一轮应该看到什么变化？ |

下一轮的 Verify 同时验证两件事：
1. 结果好不好？
2. 假设对不对？

### 对抗位

执行 Loop 的 agent 有"通过验证"的动机，容易贪婪地选择特例化方案（过拟合）。需要引入**没有 context 污染的第三方**来审视修改：

- 这个修改是解决根本问题，还是打补丁？
- 是泛化的还是特例化的？
- 换一个输入，还有效吗？

**实现方式**：无同源 context 的 subagent、外部对抗工具等。

---

## 适用边界

| ✅ 适合 | ❌ 不适合 |
|---------|----------|
| 有硬指标的任务 | 纯主观且无法判断的任务 |
| 有外部信号的任务 | 无法量化的任务 |
| 人类愿意参与验证 | 期望全自动化 |
| 可迭代改进 | 单次决策 |

**判断标准**：能否找到外部锚定的验证方式？

---

## 已知局限

| 局限 | 说明 | 缓解 |
|------|------|------|
| AI 无法精确评估主观质量 | 这是 AI 的能力边界 | 降级为红旗检测 |
| 人类参与成本 | 人类在环需要每轮参与 | 仅在需要人类判断时使用 |
| 局部最优 | 可能陷入局部最优 | 引入对抗位审视修改方向 |
| 过拟合 | agent 贪婪地特例化 | 对抗位检测泛化性 |

---

*LDD 的价值不在于全自动化，而在于结构化人机协作。*
